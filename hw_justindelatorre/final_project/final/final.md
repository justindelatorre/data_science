#...That's What He Said
##<i>Na√Øve Bayes, Natural Language Processing, and Presidential Speeches</i>

Presented by: Jay de la Torre, GA Data Science, DAT_SF_14

[IPython Notebook](https://github.com/justindelatorre/data_science/blob/master/hw_justindelatorre/final_project/final/experimental.ipynb)
<br>
[Presentation](https://docs.google.com/presentation/d/1hqCxSM7eK4TrvJif3Q3HnzlUz_logggklqWhUgO0lkc/present?slide=id.p)

###Problem Statement
The initial objective of this project was to determine how politically liberal or conservative and president was based on text from his inaugural addresses and State of the Union speeches. Over time, however, I realized that this would be difficult due to time restrictions and the lack of an existing Natural Language Processing (NLP) package that could map liberal or conservative sentiment.

After going through a few stages of data wrangling and training and testing unsuccessful models, I eventually decided to focus on training and testing a model that could accurately classify both a president's religious and political affiliation, again based on speech text.

###Obtaining the Data
Both the inaugural address and State of the Union speech text were downloaded as <code>.txt</code> files from Project Gutenberg:
<br><br>
[Inaugural Addresses](http://www.gutenberg.org/cache/epub/925/pg925.txt)
<br>
[States of the Union](http://www.gutenberg.org/cache/epub/5050/pg5050.txt)

The religious affiliation and party data were taken from a [website on president details](http://www.infoplease.com/ipa/A0194030.html). I also planned to use [ranking data](https://en.wikipedia.org/wiki/Historical_rankings_of_Presidents_of_the_United_States#Five_Thirty_Eight_analysis) from fivethirtyeight.com, but eventually decided against it.

###Data Wrangling and Feature Engineering
The raw speech text was parsed using a number of Python packages, including <code>re</code>, <code>string</code>, <code>pandas</code>, <code>numpy</code>, <code>matplotlib.pyplot</code>, and <code>CountVectorizer</code>.

My initial strategy was more of a brute-force method. First, I extracted the president names, speech types, and speech texts, mostly with <code>re</code> and <code>string</code> methods and stored them in their own respective Python lists. I then converted these lists into arrays and passed them into <code>CountVectorizer</code> instances, then converted the outputs into <code>pandas</code> DataFrames. The resulting DataFrames contained one record for each speech (with multiple speeches for most presidents), and the features were either unigram or multigram tokens generated by <code>CountVectorizer</code>; the cells contained the token count per speech. I would eventually group these DataFrames by president to get aggregate token counts at the presidential level, rather than on a speech-by-speech basis, resulting in 42 records and thousands of features/tokens for each DataFrame.

I also attempted to create a unigram tf-idf DataFrame using the <code>TfidfVectorizer</code> module, which functions similarly to <code>CountVectorizer</code>, but instead of returning token counts, returns a tf-idf value for the token for that record.

All the DataFrames mentioned so far did not have explicit <code>min_df</code> and <code>max_df</code> values, and no <code>max_features</code> ceiling was established. Only English stop words were removed from the resulting calculations.

I changed this strategy dramatically after my first attempt trying to train and test my models (see below for more information).

My second attempt added a fourth DataFrame: multigram tf-idf values. I kept the original three DataFrame concepts, but used slightly different parsing techniques to arrive at similar final DataFrames. I also set <code>min_df</code> to <code>0.25</code> and <code>max_df</code> to <code>0.75</code>; I guessed that words appearing less frequently would be less significant (especially in tf-idf calculations), and words appearing more frequently would likely just introduce noise like stop words would.

The most significant change was the creation of additional data. I suspected that the shortcomings of my initial model were due in part to the very high feature-to-record ratio in my original DataFrames, so I created utility functions that replicated the original DataFrames, then concatenated those replicas to the originals over a number of iterations. In addition, I created a function that would randomly apply +/- 1% to all non-zero values in the tf-idf DataFrames, which I hoped would introduce enough variation in the data to create replicas within acceptable variance range. With these functions, I was able to generate DataFrames that contained 1000+ rows containing similar data to the originals.

###Exploratory Data Analysis
Exploratory analysis unearthed some interesting observations, but ultimately did not lend much valuable information for feature selection, other than implying that my original feature space was too large. For example, my initial data set showed that 'government' and 'America' were two of the most commonly used words in the presidential speeches. Since these words were likely used by every president, then they said little to nothing about each president's religious or political affiliations because they were essentially ubiquitous. By adding frequency floor and ceiling values during vectorization, I was able to make more interesting notes, like how relatively frequency 'Mexico' or 'fiscal year' was used in the speeches.

I also used <code>idmax()</code> to find out what the highest-value or -count tokens were for each religious denomination and party, but there was far too much overlap among groups to infer anything significantly useful for modeling.

Another tool I used for initial exploration was <code>[WordCloud](https://github.com/amueller/word_cloud)</code>, a package that allows for the flexible creation of word cloud-like visualizations based on token frequency. I only played with it briefly, but will likely continue to explore its functionality in future iterations of this project and others.



###Training, Testing, and Validating Models

###Challenges and Triumphs

###Lessons Learned
