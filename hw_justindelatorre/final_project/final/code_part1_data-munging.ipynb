{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part I: Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Sources:</b>\n",
    "<ul>\n",
    "<li>Inaugural Addresses and States of the Union: Project Gutenberg</li>\n",
    "<li>[Presidential Data](http://www.infoplease.com/ipa/A0194030.html): Infoplease</li>\n",
    "<li>[Presidential Rankings](https://en.wikipedia.org/wiki/Historical_rankings_of_Presidents_of_the_United_States#Five_Thirty_Eight_analysis): Wikipedia/538</li>\n",
    "</ul>\n",
    "\n",
    "Structured data can be found [here](https://docs.google.com/spreadsheets/d/1cujFV5JLRivY-k6LMEDCP8_zapHUtwNCdb9Qr8h2gOQ/edit#gid=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<i>Step 1: Parsing Speech Text</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the packages we'll need to clean the data:\n",
    "<ul>\n",
    "<li><code>re</code> for regular expression functions</li>\n",
    "<li><code>pprint</code> to make printing more readable</li>\n",
    "<li><code>string</code> to clean string values</li>\n",
    "<li><code>pandas</code> because <i>duh</i></li>\n",
    "<li><code>numpy</code> because math</li>\n",
    "<li><code>matplotlib.pyplot</code> for charts</li>\n",
    "<li><code>CountVectorizer</code> for parsing tokens and removing stop words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import pprint as pp\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll open the text files and read them into Python objects that can be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inaugural Address text\n",
    "inaugural = open('../data/inaugural.txt', 'r')\n",
    "inaugural_text = inaugural.read()\n",
    "\n",
    "# State of the Union text\n",
    "sotu = open('../data/sotu.txt', 'r')\n",
    "sotu_text = sotu.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll parse the inaugural speech data using <code>re</code> modules. We'll begin by creating a list of speech titles which will act as speech IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech_id_list = re.findall(r'\\*\\s\\*\\s\\*\\s\\*\\s\\*([\\w\\s\\,\\.]+)ADDRESS',\n",
    "                                inaugural_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a <code>string</code> method (<code>strip</code>) to remove extraneous characters from the title list first. Later, we'll create a <code>dict</code> object that will have each title as a key and each full speech text as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stripped_id_list = [string.strip(title, \"\\r\\n \") for title in raw_speech_id_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to cleaning the speech text since we've cleaned the titles.\n",
    "\n",
    "All the speeches in the text file are separated by \\* \\* \\* \\* \\* delimiters, so we'll use <code>re.split</code> again to extract all the text between the delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech = re.split(r'\\*\\s\\*\\s\\*\\s\\*\\s\\*', inaugural_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use <code>re.sub</code> to replace the \"Transcriber's Notes\" because we only want the speech text for each inaugural address. We'll also ignore the first and last elements in the <code>raw_speech</code> list because it isn't actually speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "speeches = [re.sub(r'^([\\w\\W\\s]+)\\]', \"\", speech) for speech in raw_speech[1:len(raw_speech)-1]]\n",
    "\n",
    "print len(speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use a combination of <code>re.sub</code> and <code>string.strip</code> to clean up all the extra spaces and newline characters in each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "clean_speeches = []\n",
    "[clean_speeches.append(re.sub(r'\\r\\n',\n",
    "                              \" \",\n",
    "                              string.strip(speech,\n",
    "                                           \"\\r\\n\"))) \n",
    " for speech in speeches]\n",
    "\n",
    "print len(clean_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of the works is done, but you'll see that the last three speeches still contain extranous test (mostly speech IDs) that should be removed, so we'll take the last use <code>re.sub</code> on the last three to extract the last bit of cruft before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_speeches_inaugural = [re.sub(r'([A-Z0-9\\,\\.\\s]+)\\s{3}', \"\", speech) \n",
    "                            for speech in clean_speeches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inaugural data is clean, let's follow similar steps to clean the State of the Union (SOTU) speeches. Again, we'll use <code>re</code> modules to extract the text.\n",
    "\n",
    "First, we'll create a list of titles that will serve as speech IDs. Rather than extracting using Python, however, it'll be easier to just copy and paste the SOTU titles and load it into a Python list :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEORGE WASHINGTON, STATE OF THE UNION ADDRESS',\n",
      " 'GEORGE WASHINGTON, STATE OF THE UNION ADDRESS']\n"
     ]
    }
   ],
   "source": [
    "raw_speech_id_list_sotu = [\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'Zachary Taylor, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Warren Harding, State of the Union Address',\n",
    "'Warren Harding, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address'\n",
    "]\n",
    "\n",
    "# Capitalize speech IDs to conform to Inaugural Address data\n",
    "raw_speech_id_list_sotu_caps = []\n",
    "[raw_speech_id_list_sotu_caps.append(item.upper()) for item in raw_speech_id_list_sotu]\n",
    "\n",
    "pp.pprint(raw_speech_id_list_sotu_caps[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEORGE WASHINGTON, STATE OF THE UNION',\n",
      " 'GEORGE WASHINGTON, STATE OF THE UNION']\n"
     ]
    }
   ],
   "source": [
    "# Parse out speech IDs and append them to a list\n",
    "speech_id_list_sotu = []\n",
    "[speech_id_list_sotu.append(re.findall(r'^(.*?)\\sADDRESS',\n",
    "                                       speech)[0])\n",
    " for speech in raw_speech_id_list_sotu_caps]\n",
    "\n",
    "pp.pprint(speech_id_list_sotu[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the speech IDs into a single list\n",
    "title_list = stripped_id_list + speech_id_list_sotu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the hard part: let's grab the actual speech text for each State of the Union speech. First, we'll split the full text file; each speech is separated by \\*\\*\\*, so we'll split using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech_sotu = re.split(r'\\*\\*\\*\\r\\n\\r\\n', sotu_text)\n",
    "\n",
    "# Actual speeches start at index 4 and end at index -3\n",
    "raw_speech_sotu = raw_speech_sotu[4:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean things up just a bit more, we'll remove the title information in each speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "clean_speeches_2 = []\n",
    "[clean_speeches_2.append(re.findall(r'[0-9]{4}([\\w\\W\\s\\S]+)$',\n",
    "                            speech)[0])\n",
    "                            for speech in raw_speech_sotu]\n",
    "\n",
    "print len(clean_speeches_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still need to clean SOTU speeches and remove '\\r\\n' instances and replace with '' or spaces\n",
    "clean_speeches_sotu = []\n",
    "\n",
    "for speech in clean_speeches_2:\n",
    "    clean_speeches_sotu.append(re.sub(r'\\r\\n{1}', ' ', speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both sets of speeches have been properly cleaned, we'll add them both together to create an aggregate list of cleaned speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_speeches_all = clean_speeches_inaugural + clean_speeches_sotu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Tokenization with CountVectorizer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create both a unigram and multigram (bigram and trigram) <code>DataFrame</code> for non-stemmed tokens that can be found in the speeches.\n",
    "\n",
    "We'll also lowercase all the tokens, and ensure that the document frequency is between 10 and 90 percent. Words that appear in fewer than 10 percent of speeches probably aren't relevant, and words that appear in greater than 90 percent are likely stop word-like, so don't add any meaningful context to the speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a unigram vector\n",
    "unigram_vect = CountVectorizer(decode_error = 'ignore',\n",
    "                               stop_words = 'english',\n",
    "                               lowercase = True,\n",
    "                               max_features = 10000,\n",
    "                               min_df = 0.1,\n",
    "                               max_df = 0.9)\n",
    "unigram_vect.fit(clean_speeches_all)\n",
    "unigram_raw_feature_names = [token.encode('ascii','ignore') for token in unigram_vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a multigram (bigram, trigram) vector to cut total number of features\n",
    "multigram_vect = CountVectorizer(decode_error = 'ignore',\n",
    "                                 stop_words = 'english',\n",
    "                                 ngram_range = (2,3),\n",
    "                                 lowercase = True,\n",
    "                                 max_features = 10000,\n",
    "                                 min_df = 0.1,\n",
    "                                 max_df = 0.9)\n",
    "multigram_vect.fit(clean_speeches_all)\n",
    "multigram_raw_feature_names = [token.encode('ascii','ignore') for token in multigram_vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3596\n",
      "413\n"
     ]
    }
   ],
   "source": [
    "print len(unigram_raw_feature_names)\n",
    "print len(multigram_raw_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bigrams and trigrams in conjunction with setting document frequency values results in a feature space about a tenth as large the the space for unigrams only! We'll keep both instances though, since some of the techniques we'll use below seem to be more effective with unigrams, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create unigram document-term matrix, then unigram DataFrame\n",
    "unigram_dtm = unigram_vect.transform(clean_speeches_all)\n",
    "unigram_dtm.toarray()\n",
    "unigram_df = pd.DataFrame(unigram_dtm.toarray(),\n",
    "                          columns=unigram_vect.get_feature_names())\n",
    "\n",
    "# Next, make sure to only include actual words in final DataFrame before adding speech IDs\n",
    "\n",
    "# Find the index of '90', the last non-word feature\n",
    "np.where(unigram_df.columns.values == '90') # index: 61\n",
    "\n",
    "# Create DataFrame that only contains non-word features\n",
    "unigram_words_df = unigram_df.iloc[:,62:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act congress</th>\n",
       "      <th>act march</th>\n",
       "      <th>act passed</th>\n",
       "      <th>action congress</th>\n",
       "      <th>action taken</th>\n",
       "      <th>acts congress</th>\n",
       "      <th>administration government</th>\n",
       "      <th>agricultural products</th>\n",
       "      <th>amendment constitution</th>\n",
       "      <th>american citizen</th>\n",
       "      <th>...</th>\n",
       "      <th>world war</th>\n",
       "      <th>worthy consideration</th>\n",
       "      <th>year ago</th>\n",
       "      <th>year ending</th>\n",
       "      <th>year ending 30th</th>\n",
       "      <th>year ending june</th>\n",
       "      <th>year year</th>\n",
       "      <th>years ago</th>\n",
       "      <th>years come</th>\n",
       "      <th>young men</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 399 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   act congress  act march  act passed  action congress  action taken  \\\n",
       "0             0          0           0                0             0   \n",
       "1             0          0           0                0             0   \n",
       "2             0          0           0                0             0   \n",
       "3             0          0           0                0             0   \n",
       "4             0          0           0                0             0   \n",
       "\n",
       "   acts congress  administration government  agricultural products  \\\n",
       "0              0                          0                      0   \n",
       "1              0                          1                      0   \n",
       "2              0                          0                      0   \n",
       "3              0                          0                      0   \n",
       "4              0                          0                      0   \n",
       "\n",
       "   amendment constitution  american citizen    ...      world war  \\\n",
       "0                       0                 0    ...              0   \n",
       "1                       0                 0    ...              0   \n",
       "2                       0                 0    ...              0   \n",
       "3                       0                 0    ...              0   \n",
       "4                       1                 0    ...              0   \n",
       "\n",
       "   worthy consideration  year ago  year ending  year ending 30th  \\\n",
       "0                     0         0            0                 0   \n",
       "1                     0         0            0                 0   \n",
       "2                     0         0            0                 0   \n",
       "3                     0         0            0                 0   \n",
       "4                     0         0            0                 0   \n",
       "\n",
       "   year ending june  year year  years ago  years come  young men  \n",
       "0                 0          0          0           0          0  \n",
       "1                 0          0          0           0          0  \n",
       "2                 0          1          0           0          0  \n",
       "3                 0          0          0           0          0  \n",
       "4                 0          0          0           0          0  \n",
       "\n",
       "[5 rows x 399 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create multigram document-term matrix, then multigram DataFrame\n",
    "multigram_dtm = multigram_vect.transform(clean_speeches_all)\n",
    "multigram_dtm.toarray()\n",
    "multigram_df = pd.DataFrame(multigram_dtm.toarray(),\n",
    "                            columns=multigram_vect.get_feature_names())\n",
    "\n",
    "# Next, make sure to only include actual word combinations in final DataFrame before adding speech IDs\n",
    "\n",
    "# Find the index of '500 000', the last non-word feature\n",
    "np.where(multigram_df.columns.values == '500 000') # index: 13\n",
    "\n",
    "# Create DataFrame that only contains non-word features\n",
    "multigram_words_df = multigram_df.iloc[:,14:]\n",
    "\n",
    "multigram_words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Topic Clustering with Latent Dirichlet Allocation (LDA)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: constitution union state powers rights executive duty\n",
      "Topic 1: congress treaty act year duties minister territory\n",
      "Topic 2: years congress economy tax federal support programs\n",
      "Topic 3: 000 department work years increase year commission\n",
      "Topic 4: year 000 value fiscal currency report silver\n",
      "Topic 5: british powers force spain millions coast vessels\n",
      "Topic 6: general treasury means department subject present measures\n",
      "Topic 7: mexico texas shall congress army mexican territory\n",
      "Topic 8: year program federal administration million dollars billion\n",
      "Topic 9: america american americans work year tonight children\n",
      "Topic 10: men law work business navy man far\n",
      "Topic 11: american make order international possible congress needs\n",
      "Topic 12: world life freedom know men america let\n",
      "Topic 13: shall best objects commerce fellow laws duties\n",
      "Topic 14: world free military strength forces today freedom\n",
      "Topic 15: law shall laws congress service legislation right\n",
      "Topic 16: present necessary long principle purpose foreign duty\n",
      "Topic 17: subject state interests service protection importance view\n",
      "Topic 18: american year congress secretary act relations general\n",
      "Topic 19: congress business present federal action tariff industry\n"
     ]
    }
   ],
   "source": [
    "# TODO: Might want to wait for \"word-only\" matrix before using LDA to eliminate noise\n",
    "# TODO: In addition, might want to use a DataFrame the groups by president to get topics by president\n",
    "\n",
    "# Turn the DataFrame into a matrix of numpy arrays, will serve as X in LDA\n",
    "unigram_df_matrix = unigram_df.as_matrix(columns=None)\n",
    "\n",
    "# Import LDA module\n",
    "import lda\n",
    "\n",
    "# Create new instance of LDA that will group into 20 topics\n",
    "# and cycle through 1000 iterations\n",
    "unigram_model = lda.LDA(n_topics=20, n_iter=1000, random_state=1)\n",
    "unigram_model.fit(unigram_df_matrix)\n",
    "unigram_topic_word = unigram_model.topic_word_\n",
    "unigram_n_top_words = 8\n",
    "\n",
    "#Note: word_list was generated in the above section on stemming\n",
    "for i, topic_dist in enumerate(unigram_topic_word):\n",
    "    unigram_topic_words = np.array(unigram_raw_feature_names)[np.argsort(topic_dist)][:-unigram_n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(unigram_topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: great britain government united states government united secretary state\n",
      "Topic 1: federal government past years soviet union state local\n",
      "Topic 2: federal government recommend congress private enterprise national defense\n",
      "Topic 3: ending 30th 30th june year ending 30th year ending\n",
      "Topic 4: june 30 fiscal year year ending ending june 30\n",
      "Topic 5: american people state union years ago federal government\n",
      "Topic 6: constitution united states constitution united branch government congress united states\n",
      "Topic 7: great britain citizens united citizens united states british government\n",
      "Topic 8: civil service foreign trade past year fiscal year\n",
      "Topic 9: house representatives fellow citizens public debt present year\n",
      "Topic 10: attention congress district columbia house representatives session congress\n",
      "Topic 11: health care social security american people years ago\n",
      "Topic 12: 000 000 500 000 10 000 great britain\n",
      "Topic 13: general government fellow citizens federal government public money\n",
      "Topic 14: american people self government government people fellow citizens\n",
      "Topic 15: fiscal year present fiscal present fiscal year public debt\n",
      "Topic 16: fiscal year billion dollars united nations armed forces\n",
      "Topic 17: report secretary public lands secretary war secretary treasury\n",
      "Topic 18: free world united nations free nations world war\n",
      "Topic 19: national government interstate commerce time war department agriculture\n"
     ]
    }
   ],
   "source": [
    "# Might want to wait for \"word-only\" matrix before using LDA to eliminate noise\n",
    "\n",
    "# Turn the DataFrame into a matrix of numpy arrays, will serve as X in LDA\n",
    "multigram_df_matrix = multigram_df.as_matrix(columns=None)\n",
    "\n",
    "# Import LDA module\n",
    "import lda\n",
    "\n",
    "# Create new instance of LDA that will group into 20 topics\n",
    "# and cycle through 1000 iterations\n",
    "multigram_model = lda.LDA(n_topics=20, n_iter=1000, random_state=1)\n",
    "multigram_model.fit(multigram_df_matrix)\n",
    "multigram_topic_word = multigram_model.topic_word_\n",
    "multigram_n_top_words = 5\n",
    "\n",
    "#Note: word_list was generated in the above section on stemming\n",
    "for i, topic_dist in enumerate(multigram_topic_word):\n",
    "    multigram_topic_words = np.array(multigram_raw_feature_names)[np.argsort(topic_dist)][:-multigram_n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(multigram_topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Stemming with PorterStemmer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2226\n"
     ]
    }
   ],
   "source": [
    "# Next is exploring PorterStemmer. Note that PorterStemmer can only work on a list of unigrams.\n",
    "\n",
    "# Import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Instantiate a new PorterStemmer object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Create Python list of tokens in DataFrame\n",
    "word_list = list(unigram_vect.get_feature_names())\n",
    "\n",
    "# Use PorterStemmer to stem the tokens\n",
    "stems = [ps.stem(token) for token in word_list]\n",
    "stems_set = set(stems) # This reduces the number of elements from the original 3596 unigrams\n",
    "stems_list = list(stems_set)\n",
    "\n",
    "print len(stems_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "# How do I count the occurrence of each stem in each speech?\n",
    "test_speech = str.split(clean_speeches_all[0])\n",
    "counter = 0\n",
    "stems_holder = []\n",
    "\n",
    "for stem in stems_list:\n",
    "    for word in test_speech:\n",
    "        if stem == ps.stem(word):\n",
    "#            print stem + ': ' + ps.stem(word)\n",
    "            counter += 1\n",
    "            stems_holder.append(stem)\n",
    "\n",
    "print counter\n",
    "\n",
    "# Note: Checking for stems in each speech doesn't work effectively\n",
    "# It might be better to create a dictionary of all the available stems for all speeches,\n",
    "#   then use ps.stem() on each word in each speech, then compare each generated stem to the stem dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Word Relevance Using TF-IDF Analysis</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'000', u'10', u'100', ..., u'young', u'youth', u'zeal'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import TfidfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Unigram tf-idf\n",
    "tfidf_vect = TfidfVectorizer(decode_error = 'ignore',\n",
    "                             stop_words = 'english',\n",
    "                             lowercase = True,\n",
    "                             max_features = 10000,\n",
    "                             min_df = 0.1,\n",
    "                             max_df = 0.9)\n",
    "tfidf_output = tfidf_vect.fit_transform(clean_speeches_all)\n",
    "\n",
    "# Turn matrix into a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_output.toarray(),\n",
    "                        columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Panelist suggestions:\n",
    "# - Use bigrams and trigrams to reduce features and add context\n",
    "# - Try tf-idf or PCA\n",
    "# - Use min_df and max_df parameters to eliminate tokens that aren't used\n",
    "#     that often or are used too often to be meaningful\n",
    "#     CountVectorizer documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# - Grid Search can help find min_df and max_df parameters for\n",
    "#     CountVectorizer: http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Part II: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part III: Training and Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
