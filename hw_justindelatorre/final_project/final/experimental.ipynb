{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part I: Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Sources:</b>\n",
    "<ul>\n",
    "<li>Inaugural Addresses and States of the Union: Project Gutenberg</li>\n",
    "<li>[Presidential Data](http://www.infoplease.com/ipa/A0194030.html): Infoplease</li>\n",
    "<li>[Presidential Rankings](https://en.wikipedia.org/wiki/Historical_rankings_of_Presidents_of_the_United_States#Five_Thirty_Eight_analysis): Wikipedia/538</li>\n",
    "</ul>\n",
    "\n",
    "Structured data can be found [here](https://docs.google.com/spreadsheets/d/1cujFV5JLRivY-k6LMEDCP8_zapHUtwNCdb9Qr8h2gOQ/edit#gid=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###<i>Step 1: Parsing Speech Text</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the packages we'll need to clean the data:\n",
    "<ul>\n",
    "<li><code>re</code> for regular expression functions</li>\n",
    "<li><code>pprint</code> to make printing more readable</li>\n",
    "<li><code>string</code> to clean string values</li>\n",
    "<li><code>pandas</code> because <i>duh</i></li>\n",
    "<li><code>numpy</code> because math</li>\n",
    "<li><code>matplotlib.pyplot</code> for charts</li>\n",
    "<li><code>CountVectorizer</code> for parsing tokens and removing stop words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import pprint as pp\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll open the text files and read them into Python objects that can be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inaugural Address text\n",
    "inaugural = open('../data/inaugural.txt', 'r')\n",
    "inaugural_text = inaugural.read()\n",
    "\n",
    "# State of the Union text\n",
    "sotu = open('../data/sotu.txt', 'r')\n",
    "sotu_text = sotu.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll parse the inaugural speech data using <code>re</code> modules. We'll begin by creating a list of speech titles which will act as speech IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech_id_list = re.findall(r'\\*\\s\\*\\s\\*\\s\\*\\s\\*([\\w\\s\\,\\.]+)ADDRESS',\n",
    "                                inaugural_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a <code>string</code> method (<code>strip</code>) to remove extraneous characters from the title list first. Later, we'll create a <code>dict</code> object that will have each title as a key and each full speech text as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stripped_id_list = [string.strip(title, \"\\r\\n \") for title in raw_speech_id_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to cleaning the speech text since we've cleaned the titles.\n",
    "\n",
    "All the speeches in the text file are separated by \\* \\* \\* \\* \\* delimiters, so we'll use <code>re.split</code> again to extract all the text between the delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech = re.split(r'\\*\\s\\*\\s\\*\\s\\*\\s\\*', inaugural_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use <code>re.sub</code> to replace the \"Transcriber's Notes\" because we only want the speech text for each inaugural address. We'll also ignore the first and last elements in the <code>raw_speech</code> list because it isn't actually speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "speeches = [re.sub(r'^([\\w\\W\\s]+)\\]', \"\", speech) for speech in raw_speech[1:len(raw_speech)-1]]\n",
    "\n",
    "print len(speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use a combination of <code>re.sub</code> and <code>string.strip</code> to clean up all the extra spaces and newline characters in each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "clean_speeches = []\n",
    "[clean_speeches.append(re.sub(r'\\r\\n',\n",
    "                              \" \",\n",
    "                              string.strip(speech,\"\\r\\n\"))) for speech in speeches]\n",
    "\n",
    "print len(clean_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of the works is done, but you'll see that the last three speeches still contain extranous test (mostly speech IDs) that should be removed, so we'll take the last use <code>re.sub</code> on the last three to extract the last bit of cruft before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_speeches_inaugural = [re.sub(r'([A-Z0-9\\,\\.\\s]+)\\s{3}', \"\", speech) \n",
    "                            for speech in clean_speeches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inaugural data is clean, let's follow similar steps to clean the State of the Union (SOTU) speeches. Again, we'll use <code>re</code> modules to extract the text.\n",
    "\n",
    "First, we'll create a list of titles that will serve as speech IDs. Rather than extracting using Python, however, it'll be easier to just copy and paste the SOTU titles and load it into a Python list :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEORGE WASHINGTON, STATE OF THE UNION ADDRESS',\n",
      " 'GEORGE WASHINGTON, STATE OF THE UNION ADDRESS']\n"
     ]
    }
   ],
   "source": [
    "raw_speech_id_list_sotu = [\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'George Washington, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'John Adams, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'Thomas Jefferson, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Madison, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'James Monroe, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'John Quincy Adams, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Andrew Jackson, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'Martin van Buren, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'John Tyler, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'James Polk, State of the Union Address',\n",
    "'Zachary Taylor, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Millard Fillmore, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'Franklin Pierce, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'James Buchanan, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Abraham Lincoln, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Andrew Johnson, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Ulysses S. Grant, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Rutherford B. Hayes, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Chester A. Arthur, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Grover Cleveland, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'Benjamin Harrison, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'William McKinley, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'Theodore Roosevelt, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'William H. Taft, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Woodrow Wilson, State of the Union Address',\n",
    "'Warren Harding, State of the Union Address',\n",
    "'Warren Harding, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Calvin Coolidge, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Herbert Hoover, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Franklin D. Roosevelt, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Harry S. Truman, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'Dwight D. Eisenhower, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'John F. Kennedy, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Lyndon B. Johnson, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Richard Nixon, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Gerald R. Ford, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Jimmy Carter, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'Ronald Reagan, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'George H.W. Bush, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'William J. Clinton, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address',\n",
    "'George W. Bush, State of the Union Address'\n",
    "]\n",
    "\n",
    "# Capitalize speech IDs to conform to Inaugural Address data\n",
    "raw_speech_id_list_sotu_caps = []\n",
    "[raw_speech_id_list_sotu_caps.append(item.upper()) for item in raw_speech_id_list_sotu]\n",
    "\n",
    "pp.pprint(raw_speech_id_list_sotu_caps[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEORGE WASHINGTON, STATE OF THE UNION',\n",
      " 'GEORGE WASHINGTON, STATE OF THE UNION']\n"
     ]
    }
   ],
   "source": [
    "# Parse out speech IDs and append them to a list\n",
    "speech_id_list_sotu = []\n",
    "[speech_id_list_sotu.append(re.findall(r'^(.*?)\\sADDRESS',\n",
    "                                       speech)[0])\n",
    " for speech in raw_speech_id_list_sotu_caps]\n",
    "\n",
    "pp.pprint(speech_id_list_sotu[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the speech IDs into a single list\n",
    "title_list = stripped_id_list + speech_id_list_sotu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to add each president's number to each of the titles in <code>title_list</code>, which will make for easier joining when we add personal details and rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write speech DataFrame data to a file\n",
    "file_df = open('../data/ids.csv', 'w')\n",
    "for row in title_list:\n",
    "    file_df.write(row)\n",
    "    file_df.write('\\n')\n",
    "\n",
    "file_df.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we'd find a way to add the actual order numbers programmatically, but since there are relatively few records in the dataset, we can just do it by hand, then re-upload the CSV with the new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th> name</th>\n",
       "      <th> speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td>  GEORGE WASHINGTON</td>\n",
       "      <td>   FIRST INAUGURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td>  GEORGE WASHINGTON</td>\n",
       "      <td>  SECOND INAUGURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 2</td>\n",
       "      <td>         JOHN ADAMS</td>\n",
       "      <td>         INAUGURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 3</td>\n",
       "      <td>   THOMAS JEFFERSON</td>\n",
       "      <td>   FIRST INAUGURAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 3</td>\n",
       "      <td>   THOMAS JEFFERSON</td>\n",
       "      <td>  SECOND INAUGURAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name             speech\n",
       "0   1   GEORGE WASHINGTON    FIRST INAUGURAL\n",
       "1   1   GEORGE WASHINGTON   SECOND INAUGURAL\n",
       "2   2          JOHN ADAMS          INAUGURAL\n",
       "3   3    THOMAS JEFFERSON    FIRST INAUGURAL\n",
       "4   3    THOMAS JEFFERSON   SECOND INAUGURAL"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load newly-tagged data\n",
    "df_with_id = pd.read_csv('../data/ids_final.csv')\n",
    "df_with_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above <code>DataFrame</code> can be concatenated to the later <code>DataFrame</code>s containing the tokens generated by parsing the speech text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the hard part: let's grab the actual speech text for each State of the Union speech. First, we'll split the full text file; each speech is separated by \\*\\*\\*, so we'll split using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_speech_sotu = re.split(r'\\*\\*\\*\\r\\n\\r\\n', sotu_text)\n",
    "\n",
    "# Actual speeches start at index 4 and end at index -3\n",
    "raw_speech_sotu = raw_speech_sotu[4:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean things up just a bit more, we'll remove the title information in each speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "clean_speeches_2 = []\n",
    "[clean_speeches_2.append(re.findall(r'[0-9]{4}([\\w\\W\\s\\S]+)$',\n",
    "                            speech)[0])\n",
    "                            for speech in raw_speech_sotu]\n",
    "\n",
    "print len(clean_speeches_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still need to clean SOTU speeches and remove '\\r\\n' instances and replace with '' or spaces\n",
    "clean_speeches_sotu = []\n",
    "\n",
    "for speech in clean_speeches_2:\n",
    "    clean_speeches_sotu.append(re.sub(r'\\r\\n{1}', ' ', speech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both sets of speeches have been properly cleaned, we'll add them both together to create an aggregate list of cleaned speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_speeches_all = clean_speeches_inaugural + clean_speeches_sotu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_speeches = pd.DataFrame(clean_speeches_all, columns = [\"speech_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_with_id.iloc[:,:1], df_speeches], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped = df_all.groupby('id', as_index = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of aggregated speech texts by president\n",
    "speeches_all = list(df_grouped.speech_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to transform the finally-cleaned data, we'll upload each president's personal details (focusing on religion and party) and aggregate rankings (as calculated by Nate Silver and fivethirtyeight.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upload personal details and rankings into DataFrames\n",
    "df_prez = pd.read_csv('../data/presidents.csv')\n",
    "df_rankings = pd.read_csv('../data/prez_rankings_538.csv')\n",
    "\n",
    "# Clean up newly-uploaded DataFrames a bit to only include important columns\n",
    "df_prez = df_prez[['id', 'party_name', 'religion']]\n",
    "df_rankings = df_rankings[['id', 'rank_aggregate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method I: Unigram Tokenization</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a unigram vector\n",
    "unigram_vect = CountVectorizer(decode_error = 'ignore',\n",
    "                               stop_words = 'english',\n",
    "                               lowercase = True,\n",
    "                               max_features = 10000,\n",
    "                               min_df = 0.25, # original: 0.1\n",
    "                               max_df = 0.75) # original: 0.9\n",
    "unigram_vect.fit(speeches_all)\n",
    "unigram_raw_feature_names = [token.encode('ascii','ignore') for token in unigram_vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([129]),)\n"
     ]
    }
   ],
   "source": [
    "# Create unigram document-term matrix, then unigram DataFrame\n",
    "unigram_dtm = unigram_vect.transform(clean_speeches_all)\n",
    "unigram_dtm.toarray()\n",
    "unigram_df = pd.DataFrame(unigram_dtm.toarray(),\n",
    "                          columns=unigram_vect.get_feature_names())\n",
    "\n",
    "# Next, make sure to only include actual words in final DataFrame before adding speech IDs\n",
    "\n",
    "# Find the index of '90', the last non-word feature\n",
    "print(np.where(unigram_df.columns.values == '99')) # index: 129\n",
    "\n",
    "# Concat unigram DataFrame to speech ID DataFrame\n",
    "unigram_df = pd.concat([df_with_id, unigram_df.iloc[:,130:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join details and rankings\n",
    "unigram_df_all = pd.merge(pd.merge(unigram_df, df_prez, on=\"id\", how=\"left\", left_index=True), df_rankings, on=\"id\", how=\"left\", left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method II: Multigram Tokenization</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a multigram (bigram, trigram) vector to cut total number of features\n",
    "multigram_vect = CountVectorizer(decode_error = 'ignore',\n",
    "                                 stop_words = 'english',\n",
    "                                 ngram_range = (2,3),\n",
    "                                 lowercase = True,\n",
    "                                 max_features = 10000,\n",
    "                                 min_df = 0.25, # original: 0.1\n",
    "                                 max_df = 0.75) # original: 0.9\n",
    "multigram_vect.fit(speeches_all)\n",
    "multigram_raw_feature_names = [token.encode('ascii','ignore') for token in multigram_vect.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([38]),)\n"
     ]
    }
   ],
   "source": [
    "# Create multigram document-term matrix, then multigram DataFrame\n",
    "multigram_dtm = multigram_vect.transform(clean_speeches_all)\n",
    "multigram_dtm.toarray()\n",
    "multigram_df = pd.DataFrame(multigram_dtm.toarray(),\n",
    "                            columns=multigram_vect.get_feature_names())\n",
    "\n",
    "# Next, make sure to only include actual word combinations in final DataFrame before adding speech IDs\n",
    "\n",
    "# Find the index of '500 000', the last non-word feature\n",
    "print(np.where(multigram_df.columns.values == '800 000')) # index: 39\n",
    "\n",
    "# Create DataFrame that only contains non-word features\n",
    "multigram_words_df = multigram_df.iloc[:,39:]\n",
    "\n",
    "# Concat unigram DataFrame to speech ID DataFrame\n",
    "multigram_df = pd.concat([df_with_id, multigram_df.iloc[:,39:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join details and rankings\n",
    "multigram_df_all = pd.merge(pd.merge(multigram_df, df_prez, on=\"id\", how=\"left\", left_index=True), df_rankings, on=\"id\", how=\"left\", left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method III: Unigram tf-idf</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Unigram tf-idf\n",
    "unigram_tfidf_vect = TfidfVectorizer(decode_error = 'ignore',\n",
    "                                     stop_words = 'english',\n",
    "                                     lowercase = True,\n",
    "                                     max_features = 10000,\n",
    "                                     min_df = 0.25, # Original: 0.1\n",
    "                                     max_df = 0.75) # Original: 0.9\n",
    "unigram_tfidf_output = unigram_tfidf_vect.fit_transform(speeches_all)\n",
    "\n",
    "# Turn matrix into a DataFrame\n",
    "unigram_tfidf_df = pd.DataFrame(unigram_tfidf_output.toarray(),\n",
    "                                columns = unigram_tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'abandoning', u'abandonment', u'abide', ..., u'zealous',\n",
       "       u'zealously', u'zone'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tfidf_df.columns.values[130:] # Word-only columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abide</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abilities</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abolished</th>\n",
       "      <th>abolishing</th>\n",
       "      <th>abolition</th>\n",
       "      <th>...</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yielding</th>\n",
       "      <th>yields</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.013341</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.014791</td>\n",
       "      <td> 0.008624</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.020574</td>\n",
       "      <td> 0.030065</td>\n",
       "      <td> 0.044372</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.028598</td>\n",
       "      <td> 0.042207</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.008905</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.008441</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.012136</td>\n",
       "      <td> 0.021229</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.041116</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.011315</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.020106</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.008782</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.024787</td>\n",
       "      <td> 0.023391</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.023446</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.006992</td>\n",
       "      <td> 0.034058</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.009372</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0.010965</td>\n",
       "      <td> 0.005682</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.004964</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.003952</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.004670</td>\n",
       "      <td> 0.013221</td>\n",
       "      <td> 0.005682</td>\n",
       "      <td> 0.013252</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.007904</td>\n",
       "      <td> 0.023101</td>\n",
       "      <td> 0.011365</td>\n",
       "      <td> 0.005297</td>\n",
       "      <td> 0.005297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         08  abandoning  abandonment     abide  abiding  abilities   abolish  \\\n",
       "0  0.000000    0.000000            0  0.000000        0   0.013341  0.000000   \n",
       "1  0.000000    0.000000            0  0.000000        0   0.000000  0.000000   \n",
       "2  0.000000    0.000000            0  0.000000        0   0.000000  0.000000   \n",
       "3  0.000000    0.000000            0  0.020106        0   0.000000  0.008782   \n",
       "4  0.010965    0.005682            0  0.000000        0   0.000000  0.004964   \n",
       "\n",
       "   abolished  abolishing  abolition    ...      yielded  yielding    yields  \\\n",
       "0   0.000000           0   0.000000    ...     0.000000  0.000000  0.014791   \n",
       "1   0.000000           0   0.000000    ...     0.000000  0.000000  0.000000   \n",
       "2   0.008905           0   0.008441    ...     0.000000  0.000000  0.012136   \n",
       "3   0.000000           0   0.000000    ...     0.024787  0.023391  0.000000   \n",
       "4   0.000000           0   0.003952    ...     0.004670  0.013221  0.005682   \n",
       "\n",
       "       york  young     youth      zeal   zealous  zealously      zone  \n",
       "0  0.008624      0  0.020574  0.030065  0.044372   0.000000  0.000000  \n",
       "1  0.000000      0  0.000000  0.028598  0.042207   0.000000  0.000000  \n",
       "2  0.021229      0  0.000000  0.041116  0.000000   0.011315  0.000000  \n",
       "3  0.023446      0  0.006992  0.034058  0.000000   0.009372  0.000000  \n",
       "4  0.013252      0  0.007904  0.023101  0.011365   0.005297  0.005297  \n",
       "\n",
       "[5 rows x 4214 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fix the 'id' indexing of this DataFrame\n",
    "unigram_tfidf_df_all = pd.concat([unigram_tfidf_df.iloc[:,:], unigram_tfidf_df.iloc[:,130:]], axis = 1)\n",
    "unigram_tfidf_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-12fe66c0cb9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO: Need to add presidential details and rankings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munigram_tfidf_df_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram_tfidf_df_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_prez\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_rankings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0munigram_tfidf_df_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy)\u001b[0m\n\u001b[0;32m     36\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                          copy=copy)\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy)\u001b[0m\n\u001b[0;32m    182\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    183\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                     \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m                     \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0m_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1778\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1780\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1782\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1785\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   2847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2849\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2850\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2851\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[0mloc\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0munique\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly\u001b[0m \u001b[0mslice\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \"\"\"\n\u001b[1;32m-> 1402\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3820)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3700)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12323)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12274)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# TODO: Need to add presidential details and rankings\n",
    "unigram_tfidf_df_all = pd.merge(pd.merge(unigram_tfidf_df_all, df_prez, on=\"id\", how=\"left\", left_index=True), df_rankings, on=\"id\", how=\"left\", left_index=True)\n",
    "unigram_tfidf_df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method IV: Multigram tf-idf</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8aa1a6942f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                                        \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Original: 0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                        max_df = 0.75) # Original: 0.9\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmultigram_tfidf_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultigram_tfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeeches_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Turn matrix into a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \"\"\"\n\u001b[1;32m-> 1282\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# disable defaultdict behaviour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Multigram tf-idf\n",
    "multigram_tfidf_vect = TfidfVectorizer(decode_error = 'ignore',\n",
    "                                       stop_words = 'english',\n",
    "                                       lowercase = True,\n",
    "                                       ngram_range = (2, 3),\n",
    "                                       max_features = 10000,\n",
    "                                       min_df = 0.25, # Original: 0.1\n",
    "                                       max_df = 0.75) # Original: 0.9\n",
    "multigram_tfidf_output = multigram_tfidf_vect.fit_transform(speeches_all)\n",
    "\n",
    "# Turn matrix into a DataFrame\n",
    "multigram_tfidf_df = pd.DataFrame(multigram_tfidf_output.toarray(),\n",
    "                                  columns = multigram_tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multigram_tfidf_df.columns.values[39:] # Word-only columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multigram_tfidf_df_all = multigram_tfidf_df.iloc[:,39:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Need to add presidential details and rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method V: PorterStemmed tf-idf</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: PorterStemmer > tf-idf (Unigram) > Create fake, similar data > Ensemble technique classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part II: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method I: Frequency Bar Charts</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method II: <code>idmax()</code></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method III: Latent Dirichlet Allocation (LDA)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<i>Method IV: WordCloud Module</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part III: Training and Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part IV: Applying to Outside Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
